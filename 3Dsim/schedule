目的：测试在没有GC的影响下读写性能受限条件
方法：
首先测试不同负载下的读写性能
    对于读性能
        在不受到写干扰和GC干扰时可以达到的性能
            顺序读 
            随机读
        
    对于写性能
        在不受到读干扰和GC干扰时可以达到的性能
            顺序写

请求数量、请求间隔、请求大小、请求类型(读写更新以及顺序随机)

写性能
    - 增加buffer write hit，从而降低写延迟
    - 由于读请求的优先级比写请求高，写请求的延迟也受到读请求的影响

读性能
    - 提高buffer read hit，从而降低读延迟
    - 读请求大小越大，延迟越大；猜测是请求会
    - 相同的请求数目以及请求大小，随机读请求比顺序读请求拆分的子请求数目多(?)，所以读延迟高;

看一下划分子请求的部分
再看一下age和warm出错的原因
    奇怪 age之前plane中的block0 free page即为0

首先，这个代码有问题，warm flash会导致闪存全部写满

最好是不影响读写性能的前提下提高可靠性
思考Huffman+AC是否可行
需要先弄明白Huffman对读写性能的影响有多大 以及优化空间在哪 

添加压缩功能
    记住！压缩和解压的单位要一样，所以压缩的粒度也是一个问题
    直接改变请求大小；
        可以将压缩后的数据存放到DRAM中；增加了DRAM的数据密度，由于访问DRAM的速度比访问flash的速度要快，所以可能可以提高性能，但是添加了压缩解压延迟
        也可以当flash的buffer满了之后再压缩；
            每个页到了buffer就压缩；方便对齐，避免数据跨4KB
            三个页都到了buffer再压缩；一起压缩压缩率更高，适合大粒度顺序读取的数据
    改变block中page的数量;
        直接用SLC模拟Huffman编码后的数据，将压缩后的数据大小除以3
    修改读请求延迟;
    修改传输延迟;
    添加压缩解压延迟（分为数据从flash中读和从dram中读） 
    （可能会出现由于对齐导致的数据膨胀）

initiation
pre_process_page
warm_flash
make_aged
    有问题
simulate
    get_requests
        得到请求之后插入ssd->request_queue
        0: 到了trace尾部
        1: 添加请求到ssd->request_queue
        -1:没有添加请求到ssd->request_queue
    buffer_management/no_buffer_distribute
        no_buffer_distribute 有问题
        从ssd->request_work（ssd->request_queue最近一个请求）中得到请求
        check_w_buff
            未命中，去flash上读；完全命中；部分命中（命中部分在buffer，其他去flash读）
            需要去flash读部分（creat_sub_request）
            将command_buffer->buffer_tail的子请求sub_req挂载在总请求req上，为请求的执行完成做准备
            在读操作的情况下，先要预先判断读子请求队列ssd->channel_head[loc->channel].subs_r_head中是否有与这个子请求相同的，有的话，新子请求就不必再执行了，将新的子请求直接赋为完成；没有则要把子请求插入到对应的channel队列   
        insert2buffer 查找数据在DRAM中是否存在，不在或者部分不存在都需要把数据插入到DRAM中；但当DRAM空间不足则需要将DRAM中数据写入到flash中，具体操作是把写入请求写入到对应的command buffer中；当然这个过程也是先检查是否有重复，重复则更新，不重复则插入；当command buffer满了则将command buffer中的数据一次性写入到flash中；写入flash时需要先检查该逻辑地址是否写入过，若有则需要先读再写入，否则直接写入
            按照逻辑页面地址在dram空间ssd->dram->buffer查找是否命中，三种情况：完全命中、部分命中和不命中
                将新的节点设置为dram的buffer_head
                DRAM空间是以sector为单位管理的，如果写入数据超过了DRAM最大容量，需要将dram的buffer_tail的数据写入到flash中（distribute2_command_buffer），直到空余空间可以写入新请求数据
                    需要根据lpn和分配方式进行*allocation_method
                        决定command写入的buffer为ssd->dram->command_buffer(支持高级命令的buffer)还是ssd->dram->static_die_buffer
                        并且按照分配方式决定请求写入的channel、chip、plane、die
                        按照分配方式决定请求挂载在channel上还是ssd上
                            混合分配
                                动态负载感知（注释掉了）
                                静态负载感知
                                    根据逻辑地址在DRAM中读写请求次数；
                                    读多：aim_command_buffer（static_die_buffer）；按照stripe的方式更新
                                    写多：aim_command_buffer（command_buffer）；按照聚集的方式更新？逻辑页面地址 mount type是什么？
                            静态分配
                                请求分配的物理地址只跟逻辑地址相关，按照逻辑地址计算分配到的channel、chip以及die
                                并计算请求分配到的buffer（static_die_buffer），mount_flag为CHANNEL_MOUNT
                            动态分配
                                stripe：aim_command_buffer（static_die_buffer）；全局die编码，当每个die的plane写满之后更新die编码（每个新的请求plane count都会增加，即plane计数是以页面为单位的）
                                ospa ：aim_command_buffer（static_die_buffer）；选择写入的die位置，如果有die对应的命令队列为空则选择该die写入，否则遍历所有的die_buffer并计算欧式距离，返回对应die中最小的欧式距离
                                poll ：aim_command_buffer（static_die_buffer）；首先获取轮询的分配的令牌，重新轮询到一个新的die上，计算当前aim-die的距离是否等于1，连续的两个页分配到相同的die缓存上
                                other ：aim_command_buffer（command_buffer）
                    然后根据lpn进行insert2_command_buffer
                        在command写入的buffer查找是否有重复的请求，三种情况：完全命中、部分命中和不命中
                        生成一个buff_cmd_node,根据这个页的情况分别赋值给各个成员，并且添加到队首
                        如果缓存已满，此时发生flush操作，将缓存的内存一次性flush到闪存上
                            （creat_sub_request）
                            将command_buffer->buffer_tail的子请求sub_req挂载在总请求req上，为请求的执行完成做准备
                            写请求的情况下，就需要利用到函数allocate_location(ssd ,sub)来处理静态分配和动态分配
                                判断是否会产生更新写操作，更新写操作要先读(更新channel的subs_r队列)后写
                                    ？？为什么当更新产生的读请求在channel的读队列则将请求的当前时间设置为SSD的当前时间（对这个时间存疑），并且将读完成时间设置为读一次DRAM的时间
                                按照不同的分配策略，进行分配allocation_method
                                    混合分配 mount_type 0:000；1:010；2:100:3:110；4:-1-1-1
                                    静态 PLANE_STATIC_ALLOCATION/SUPERPAGE_STATIC_ALLOCATION：
                                        CHANNEL_PLANE_STATIC_ALLOCATION/CHANNEL_SUPERPAGE_STATIC_ALLOCATION：
                                然后根据mount_flag, 选择挂载在channel上还是ssd的写请求队列
    process
        gc
        services_2_r_read
            当前CHIP_READ_BUSY？？，find_read_sub_request从ssd->channel_head[channel].subs_r_head找到SR_R_READ的子请求（*需要修改）
                按照ONE_SHOT_READ_MUTLI_PLANE、ONE_SHOT_READ、MUTLI_PLANE、NORMAL的顺序找读子请求并执行
            go_one_step 当前状态是SR_R_READ下一个阶段是SR_R_DATA_TRANSFER（*需要修改）
                NORMAL、ONE_SHOT_READ_MUTLI_PLANE、ONE_SHOT_READ、MUTLI_PLANE、AD_HALFPAGE_READ
                相同page在不同plane中的数据不同，压缩比不同，需要传输的时间不一样
        services_2_r_complete
            在channel级别查找结束的sub request（SR_COMPLETE），从subs_r_head中移除并挂载到channel_head->subs_r_tail中
        services_2_r_wait   
            resume_erase_operation
            find_r_wait_sub_request 
                这里的调度是为队列subs_r_head第一个请求按照操作的优先级找到当前状态为SR_WAIT的子请求集合(find_r_wait_sub_request)
                    如果读请求个数很多，每次匹配最近入队的请求，那中间的请求等待的时间会很长？
                    应该综合考虑请求等待时间以及并行度和单次读延迟？
            go_one_step 当前状态是SR_R_C_A_TRANSFER下一个阶段是SR_R_READ
                在SR_R_C_A_TRANSFER阶段会给子请求赋予读类型（NORMAL、ONE_SHOT_READ_MUTLI_PLANE、ONE_SHOT_READ、MUTLI_PLANE、AD_HALFPAGE_READ）
        services_2_r_data_trans
            find_read_sub_request（CHIP_DATA_TRANSFER）
            go_one_step 当前状态是SR_R_DATA_TRANSFER下一个阶段是SR_COMPLETE
                （需要修改数据传输的数据）
        services_2_write 在channel级别
            DYNAMIC_ALLOCATION or HYBRID_ALLOCATION：从channel对应的token开始遍历channel中的每个chip
            STATIC_ALLOCATION：按照chip的编号0到N遍历channel中的每个chip，当前chip为空闲，则执行写命令
                dynamic_advanced_process（CHIP_IDLE）
                遍历请求链上当前空闲的请求
                如果是静态分配，当前请求属于同一个channel,chip,die则保留
                如果是动态分配，保留
                    service_advance_command or get_ppn_for_normal_command
                        get_ppn_for_advanced_commands 
                            ONE_SHOT_MUTLI_PLANE
                                find_level_page (*需要修改)找到不同die中相同偏移量的page以执行multi plane操作（原本是一次调用multi plane并行写入一个页面，现在一次调用find_level_WL，一次调用写入3个页面）
                                    先find_active_block 
                                    判断所有的page是否相等，如果相等，执行flash_page_state_modify（*需要修改）将原本一次修改一个页面的映射关系改为一次修改一个WL对应的三个页面的映射关系
                                    如果不相等，贪婪的使用，将所有的page向最大的page靠近
                            ONE_SHOT
                                get_ppn
                            compute_serve_time
                        get_ppn_for_normal_command
    trace_output
        将ssd->request_queue中执行完的请求释放
statistic_output

请求会被划分为page然后写入，从dram中以页面为单位写入，flush到ssd中也是以page为单位的
首先，读取的粒度和压缩的粒度相同，是按照请求的大小进行压缩还是按照分页的大小进行压缩；
    按照分页的大小进行压缩
齐次，压缩对外提供逻辑地址空间是否需要变化


CRRC的方法对小的读取效果会很差


读性能优化：
    one-shot-multi-plane 3*tR*0.8
    one-shot-read 3*tR*0.8
    multi-plane tR
    normal tR
    half-page 0.5*tR
CRRC读延迟增加原因：
    一则是没有了half-page read、one-shot-read、one-shot-multi-plane read，CRRC只支持multiplane和normal
    二则加了压缩解压延迟
    三则需要读三个页面才能读出数据

有问题，原本代码中ONE_SHOT_READ的延迟为tR*0.8；



需要搞清楚的问题：
读操作为多个充电时间、评价时间、放电时间，这个时间和施加的电压有关吗？
multi-plane的电源如何如何充电的？？？

原始代码问题：
fcl.c中ssd->one_shot_read_count加了两次，一次在services_2_r_data_trans一次在go-one-step

问题：
1、查看了完全顺序读的负载
read request average size:     95.899559 = 16*6 sectors =16*3 KB
512M_16KB_random_R.ascii 间隔平均延迟: 5.00886e+06 ns 5008.86 us, Percentile: 2000,000 5000,000 8000,000 10000,000
现象：
总共299684个读的子请求
huffman Static 1:OSR 31182；MP 18285；MP-OSR 29；normal 169394，L=713766
baseline Static 0:OSR 15780；MP 56048；MP-OSR 0；normal 140248，L=412174
baseline Static 1:OSR 27889；MP 12249；MP-OSR 13；normal 191441，L=442175
baseline Static 2:OSR 35；MP 7357；MP-OSR 0；normal 284865，L=265836（通过channel级别的并行读取减少延迟）
baseline Static 3:OSR 21；MP 3；MP-OSR 0；normal 299615，L=272036
分析负载：请求的平均大小为3个页面，且平均延迟间隔大（远超过请求读取的延迟）
原因1：MP-OSR数目太少，仍然存在很多normal读，猜测是队列的请求数目太少
解决方法1:
当channel级别的读请求个数少于3则不执行请求
huffman:OSR 78435；MP 1668；MP-OSR 9700；normal 2843，L=4207973
当channel级别的读请求个数少于6则不执行请求
huffman:OSR 4286；MP 1700；MP-OSR 46839；normal 2392，L=7290381
结果不理想,由于读延迟即使是一次性读 为了凑齐高级命令子请求等待的时间长；
请求平均可以划分为6个子请求，那为什么会等待时间长？

对于顺序读请求：
首先，要先明确数据确实是我想要的形式布局，即相邻的数据尽量存储在同一个plane以及同一个die中123/456，确定！
其次，在执行读请求时，会查找包括队列的第一个子请求的多个满足同一个高级命令的多个读子请求；
    不合理之处包括：
    1、处理读请求时目标是处理最先到达channel的请求，按照一定优先级查找执行高级命令的子请求集合
    当channel有请求sub channel chip die plane block pagewl pagenumber
                   0    1       1   0   0       0   0       2
                   1    1       1   0   1       0   0       0
                   2    1       1   0   1       0   0       1
                   3    1       1   0   1       0   0       2 
    由于sub0最先到达会和plane1中的sub3一起执行multi-plane,而不是sub1、2、3一起执行OSR
    2、读字线上的任意页面都只需要一次读OSR就行；对于Multi-plane—OSR，只要不同Plane的相同位置的WL有请求即可;

    解决方法：
    直接采用独立plane
    在查找读请求执行时，先查找

    
另外，添加独立plane执行功能
还有考虑预取以及读传输数据粒度的问题

首先，利用一次性读的目的是为了减少读取一个页面需要占用的flash时间
使用一次性读，全顺序读延迟 713766
不使用一次性读 819939
其次，由于Huffman编码采用具体的某个闪存状态组合表示数据，在读取数据时都需要对目标字线施加（N-1）次电压，
当激活位线数目少时，读取延迟会相应减少；当激活WL对应的所有位线时，施加N-1次电压可以读出多个页面的数据；
    我们先探索充分利用后者可以带来的好处，如何充分利用？
    目的是减少读放大，一则可以将读出来的数据放在DRAM中，便于后续顺序读取；二则可以考虑数据布局

N-LC对比原本读写的好处为：写延迟减少！读取三个页面的延迟减少！
缺点：读取单个页面时间增加
SOML read 延迟设定
4KB 45us 8KB 65us 12KB 85us 16KB 99us

适用场景：
读写密集型
    因为如果请求之间的间隔长且单个请求大小不大则通过并行能更好地提升读写性能（在不考虑GC的前提下）
    在考虑GC的前提下，由于相邻逻辑地址的数据尽量存放在一起，所以寿命相近的数据会一起被GC，从而减少了GC对读写性能的影响
对于纯读负载：
    顺序读
        在请求大(>48KB)且请求间隔较小(<270000)的负载可以充分利用OSR；否则不行
    随机读


本周任务：
    了解下这个时间是如何推进的
    1、实现IMP 
    2、实现SOML read 
    3、实现PR

    问题：
    IMP是否可以实现不同请求的并行操作
    SOML read中提到的block decoder是什么？
    目前实现的顺序是什么？

    可能的动机：
    上层的读请求在flash层可能不是完全对齐的，造成部分读请求
    LPN 100 state 0001
    LPN 101 state 1111
    LPN 102 state 1100

    现在处理的逻辑
        找最早到达队列的请求执行的命令，
            
        


creat_sub_request的读有问题，有相同物理地址的读请求在队列中就返回了，没有修改队列请求的state；并且请求完成的时间为SSD的当前时间加上访问DRAM的时间
go_one_step中只是normal 命令（只涉及到一个chip中的一个die），执行SR_R_C_A_TRANSFER命令后转为CHIP-READ-BUSY状态？另外一个die可以执行独立的读写操作啊
问题：SOML read在读不同的子页时可能是不同的页面类型，施加的电压不相同
    这个是如何考虑的？
OSR有问题！
FCFS是不合理的，只考虑了到达的时间
并行度高也不好，读受到写和GC的影响大
现有的请求在执行时没有考虑到事务


首先，最大的问题是SSD读写延迟问题
读写延迟是如何计算的
    请求对应的子请求最晚完成时间-达到时间